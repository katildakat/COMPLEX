{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note to myself and others:\n",
    "I become confused easily with cased/incased transformers naming thing (cause it kinda sounds vice versa to me), so I'll use two other terms\n",
    "\n",
    "* **lowercased** model/tokenizer/string - transformed to lower case (Mom -> mom), 'uncased' in BERT\n",
    "* **propercased** model/tokenizer/string - with unchanged casing (Mom -> Mom), 'cased' in BERT\n",
    "\n",
    "# Data examination\n",
    "UPDATE A problem: \n",
    "* There is at least one target that is a part of a longer string (it is glued to other characters). It is not demonstrated in target coulmn, and BERT tokenizes it with '##' appended\n",
    "* Question 5: how to deal with it?\n",
    "\n",
    "Questions:\n",
    "* 1. how many words are there in each subcorpus? (single words vs mwe / bible vs biomed vs europarl)\n",
    "* 2. are there multiple target word instances in a sentence?\n",
    "\n",
    "**these are kinda outdated after a '##N' problem was encountered**\n",
    "* 3. are there any words not in BERT vocab? (are there words that are segmented futher?)\n",
    "\n",
    "checking just a target word type being present in BERT's vocab (was a bad idea)\n",
    "\n",
    "* 4. are there any oov subwords in oov words?\n",
    "\n",
    "SUMMARY:\n",
    "* there are no duplicates inside a subcorpus and cross train-trail when looking at ids\n",
    "* **case** is important to identify target token position in a sentence, it is straitforward with propercased sentences.\n",
    "* with propercased sentences only rows 6144 and 6200 in single train have double occurances of target words\n",
    "* not all tokens are present in BERT vocabulary (represent as an average of their subwords?)\n",
    "* there are no UNKs when target words are segmented, there are some UNKs in sentences of singles corpora\n",
    "* lowercased BERT has less unsegmented tokens than propercased (maybe case can be used to id target word in a sentence, but then lowercased and represented?)\n",
    "* There should be several tests to find BERT tokens that represent targets (target as a whole, target subwords as the sublist of sentence tpokens, target as a whole as the substring of a token in tokenized sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(file_name):\n",
    "    df = pd.read_csv(file_name, '\\t', quoting=3, na_filter=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = read_tsv('data/train/lcp_single_train.tsv')\n",
    "multi_train = read_tsv('data/train/lcp_multi_train.tsv')\n",
    "\n",
    "single_trial = read_tsv('data/trial/lcp_single_trial.tsv')\n",
    "multi_trial = read_tsv('data/trial/lcp_multi_trial.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3ZLW647WALVGE8EBR50EGUBPU4P32A</td>\n",
       "      <td>bible</td>\n",
       "      <td>Behold, there came up out of the river seven c...</td>\n",
       "      <td>river</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34R0BODSP1ZBN3DVY8J8XSIY551E5C</td>\n",
       "      <td>bible</td>\n",
       "      <td>I am a fellow bondservant with you and with yo...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</td>\n",
       "      <td>bible</td>\n",
       "      <td>The man, the lord of the land, said to us, 'By...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id corpus  \\\n",
       "0  3ZLW647WALVGE8EBR50EGUBPU4P32A  bible   \n",
       "1  34R0BODSP1ZBN3DVY8J8XSIY551E5C  bible   \n",
       "2  3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  bible   \n",
       "\n",
       "                                            sentence     token  complexity  \n",
       "0  Behold, there came up out of the river seven c...     river        0.00  \n",
       "1  I am a fellow bondservant with you and with yo...  brothers        0.00  \n",
       "2  The man, the lord of the land, said to us, 'By...  brothers        0.05  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3S37Y8CWI80N8KVM53U4E6JKCDC4WE</td>\n",
       "      <td>bible</td>\n",
       "      <td>but the seventh day is a Sabbath to Yahweh you...</td>\n",
       "      <td>seventh day</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3WGCNLZJKF877FYC1Q6COKNWTDWD11</td>\n",
       "      <td>bible</td>\n",
       "      <td>But let each man test his own work, and then h...</td>\n",
       "      <td>own work</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3UOMW19E6D6WQ5TH2HDD74IVKTP5CB</td>\n",
       "      <td>bible</td>\n",
       "      <td>To him who by understanding made the heavens; ...</td>\n",
       "      <td>loving kindness</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id corpus  \\\n",
       "0  3S37Y8CWI80N8KVM53U4E6JKCDC4WE  bible   \n",
       "1  3WGCNLZJKF877FYC1Q6COKNWTDWD11  bible   \n",
       "2  3UOMW19E6D6WQ5TH2HDD74IVKTP5CB  bible   \n",
       "\n",
       "                                            sentence            token  \\\n",
       "0  but the seventh day is a Sabbath to Yahweh you...      seventh day   \n",
       "1  But let each man test his own work, and then h...         own work   \n",
       "2  To him who by understanding made the heavens; ...  loving kindness   \n",
       "\n",
       "   complexity  \n",
       "0    0.027778  \n",
       "1    0.050000  \n",
       "2    0.050000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subcorpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3QI9WAYOGQB8GQIR4MDIEF0D2RLS67</td>\n",
       "      <td>bible</td>\n",
       "      <td>They will not hurt nor destroy in all my holy ...</td>\n",
       "      <td>sea</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3T8DUCXY0N6WD9X4RTLK8UN1U929TF</td>\n",
       "      <td>bible</td>\n",
       "      <td>that sends ambassadors by the sea, even in ves...</td>\n",
       "      <td>sea</td>\n",
       "      <td>0.102941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3I7KR83SNADXAQ7HXK7S7305BYB9KD</td>\n",
       "      <td>bible</td>\n",
       "      <td>and they entered into the boat, and were going...</td>\n",
       "      <td>sea</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id subcorpus  \\\n",
       "0  3QI9WAYOGQB8GQIR4MDIEF0D2RLS67     bible   \n",
       "1  3T8DUCXY0N6WD9X4RTLK8UN1U929TF     bible   \n",
       "2  3I7KR83SNADXAQ7HXK7S7305BYB9KD     bible   \n",
       "\n",
       "                                            sentence token  complexity  \n",
       "0  They will not hurt nor destroy in all my holy ...   sea    0.000000  \n",
       "1  that sends ambassadors by the sea, even in ves...   sea    0.102941  \n",
       "2  and they entered into the boat, and were going...   sea    0.109375  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_trial.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subcorpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31HLTCK4BLVQ5BO1AUR91TX9V9IVGH</td>\n",
       "      <td>bible</td>\n",
       "      <td>The name of one son was Gershom, for Moses sai...</td>\n",
       "      <td>foreign land</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389A2A304OIXVY7G5B71Q9M43LE0CL</td>\n",
       "      <td>bible</td>\n",
       "      <td>unleavened bread, unleavened cakes mixed with ...</td>\n",
       "      <td>wheat flour</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31N9JPQXIPIRX2A3S9N0CCFXO6TNHR</td>\n",
       "      <td>bible</td>\n",
       "      <td>However the high places were not taken away; t...</td>\n",
       "      <td>burnt incense</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id subcorpus  \\\n",
       "0  31HLTCK4BLVQ5BO1AUR91TX9V9IVGH     bible   \n",
       "1  389A2A304OIXVY7G5B71Q9M43LE0CL     bible   \n",
       "2  31N9JPQXIPIRX2A3S9N0CCFXO6TNHR     bible   \n",
       "\n",
       "                                            sentence          token  \\\n",
       "0  The name of one son was Gershom, for Moses sai...   foreign land   \n",
       "1  unleavened bread, unleavened cakes mixed with ...    wheat flour   \n",
       "2  However the high places were not taken away; t...  burnt incense   \n",
       "\n",
       "   complexity  \n",
       "0    0.000000  \n",
       "1    0.157895  \n",
       "2    0.200000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_trial.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming a column\n",
    "single_trial = single_trial.rename(columns={'subcorpus':'corpus'})\n",
    "multi_trial = multi_trial.rename(columns={'subcorpus':'corpus'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# checking that there are no duplicates inside a subcorpus\n",
    "print(len(single_train['id'].unique()) == len(single_train['id']))\n",
    "print(len(single_trial['id'].unique()) == len(single_trial['id']))\n",
    "print(len(multi_train['id'].unique()) == len(multi_train['id']))\n",
    "print(len(multi_trial['id'].unique()) == len(multi_trial['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(any([i in single_train['id'].tolist() for i in single_trial['id'].tolist()]))\n",
    "print(any([i in multi_train['id'].tolist() for i in multi_trial['id'].tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "## how many words are there in each subcorpus? (single words vs mwe / bible vs biomed vs europarl)\n",
    "According to the description (https://sites.google.com/view/lcpsharedtask2021/data?authuser=0), \n",
    "the training set includes 1,517 MWEs from 3 domains: 505 bible, 514 biomedical, and 498 Europarl and 7,662 single words: 2,574 bible, 2,576 biomedical, and 2,512 Europarl.\n",
    "\n",
    "The trial set includes 99 MWEs from 3 domains: 29 bible, 33 biomedical, and 37 Europarl, and 421 single words: 143 bible, 135 biomedical, and 143 Europarl.\n",
    "\n",
    "NOTE: there can be several sentence for the same target word\n",
    "\n",
    "NOTE2: the size of the corpus differs from what is reported in the paper (https://arxiv.org/pdf/2003.07008.pdf). For example, there are more biomed examples than reported, and there are no test set yet available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [single_train, multi_train, single_trial, multi_trial]\n",
    "names = ['single_train', 'multi_train', 'single_trial', 'multi_trial']\n",
    "bible = []\n",
    "europarl = []\n",
    "biomed = []\n",
    "total_set = []\n",
    "for df in dfs:\n",
    "    bible.append(len(df[df['corpus'] == 'bible']))\n",
    "    europarl.append(len(df[df['corpus'] == 'europarl']))\n",
    "    biomed.append(len(df[df['corpus'] == 'biomed']))\n",
    "    total_set.append(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_NAME</th>\n",
       "      <th>BIBLE_LEN</th>\n",
       "      <th>EUROPARL_LEN</th>\n",
       "      <th>BIOMED_LEN</th>\n",
       "      <th>TOTAL_LEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>single_train</td>\n",
       "      <td>2574.0</td>\n",
       "      <td>2512.0</td>\n",
       "      <td>2576.0</td>\n",
       "      <td>7662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multi_train</td>\n",
       "      <td>505.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>1517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>single_trial</td>\n",
       "      <td>143.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multi_trial</td>\n",
       "      <td>29.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL_CORPUS_LEN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3251.0</td>\n",
       "      <td>3190.0</td>\n",
       "      <td>3258.0</td>\n",
       "      <td>9699.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DATA_NAME  BIBLE_LEN  EUROPARL_LEN  BIOMED_LEN  TOTAL_LEN\n",
       "0                 single_train     2574.0        2512.0      2576.0     7662.0\n",
       "1                  multi_train      505.0         498.0       514.0     1517.0\n",
       "2                 single_trial      143.0         143.0       135.0      421.0\n",
       "3                  multi_trial       29.0          37.0        33.0       99.0\n",
       "TOTAL_CORPUS_LEN           NaN     3251.0        3190.0      3258.0     9699.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'DATA_NAME':names,'BIBLE_LEN':bible,'EUROPARL_LEN':europarl,'BIOMED_LEN':biomed,'TOTAL_LEN':total_set}\n",
    "corpora_lens = pd.DataFrame(data)\n",
    "corpora_lens.loc['TOTAL_CORPUS_LEN'] = corpora_lens.sum(numeric_only=True,axis=0)\n",
    "\n",
    "corpora_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "## are there multiple target word instances in a sentence? (Yes and No)\n",
    "with propercased sentences only 6144 and 6200 in single train are with multiple instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single words\n",
    "### train\n",
    "* when lowercased, there might be 1/2/3/4/11/9/5 occurances of a token substring in a sentence string (no specific tokenization). The most are for chemical elements (since they are just letters). there are 87 sentences with multiple target token this way. For example, in sentence 6756, it is not really clear which instance of a target word should be evaluated.\n",
    "* when propercased, there are 1/2 occurances of a token substring in a sentence string. only two sentences contain more than 1 occurance.\n",
    "\n",
    "Summary: casing is important for chosing the right target token in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of token strings in a lowercased sentence\n",
    "single_train['occurance_number_lowercased'] = single_train.apply(lambda row:\n",
    "    row['sentence'].lower().count(row['token'].lower()), axis=1\n",
    ")\n",
    "\n",
    "# counting the number of token strings in a proprcased sentence\n",
    "single_train['occurance_number_propercased'] = single_train.apply(lambda row:\n",
    "    row['sentence'].count(row['token']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4 11  9  5]\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "print(single_train['occurance_number_lowercased'].unique())\n",
    "print(single_train['occurance_number_propercased'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_train[single_train['occurance_number_lowercased']>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "      <th>occurance_number_lowercased</th>\n",
       "      <th>occurance_number_propercased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>3X4Q1O9UBHMCMY43GF110OQ80EE7O2</td>\n",
       "      <td>biomed</td>\n",
       "      <td>CorA, B, C and D belong to a protein family in...</td>\n",
       "      <td>B</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4814</th>\n",
       "      <td>3A3KKYU7P3H3CAKSB7U0000KY4FWMJ</td>\n",
       "      <td>biomed</td>\n",
       "      <td>The mice used in the present study, (NFR/N × B...</td>\n",
       "      <td>MA</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>3T5ZXGO9DEOYRKNPENLOGDE7P89QZL</td>\n",
       "      <td>biomed</td>\n",
       "      <td>Because synapsis occurs in TRIP13-deficient sp...</td>\n",
       "      <td>CO</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5079</th>\n",
       "      <td>3538U0YQ1FU0F2QNF0FL0D5E3B1F3J</td>\n",
       "      <td>biomed</td>\n",
       "      <td>Superficial and deep anterior cortical stainin...</td>\n",
       "      <td>N</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>3TTPFEFXCTKJQH4BTS1JA1TBTGIH6P</td>\n",
       "      <td>biomed</td>\n",
       "      <td>Peptide Aβ is released from APP by the action ...</td>\n",
       "      <td>N</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>36MUZ9VAE626RGSODE1RV46QINFED2</td>\n",
       "      <td>europarl</td>\n",
       "      <td>A4-0124/97 by Mr Wynn, on behalf of the Commit...</td>\n",
       "      <td>VI</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id    corpus  \\\n",
       "4203  3X4Q1O9UBHMCMY43GF110OQ80EE7O2    biomed   \n",
       "4814  3A3KKYU7P3H3CAKSB7U0000KY4FWMJ    biomed   \n",
       "4927  3T5ZXGO9DEOYRKNPENLOGDE7P89QZL    biomed   \n",
       "5079  3538U0YQ1FU0F2QNF0FL0D5E3B1F3J    biomed   \n",
       "5080  3TTPFEFXCTKJQH4BTS1JA1TBTGIH6P    biomed   \n",
       "7579  36MUZ9VAE626RGSODE1RV46QINFED2  europarl   \n",
       "\n",
       "                                               sentence token  complexity  \\\n",
       "4203  CorA, B, C and D belong to a protein family in...     B    0.400000   \n",
       "4814  The mice used in the present study, (NFR/N × B...    MA    0.593750   \n",
       "4927  Because synapsis occurs in TRIP13-deficient sp...    CO    0.526316   \n",
       "5079  Superficial and deep anterior cortical stainin...     N    0.602941   \n",
       "5080  Peptide Aβ is released from APP by the action ...     N    0.750000   \n",
       "7579  A4-0124/97 by Mr Wynn, on behalf of the Commit...    VI    0.485294   \n",
       "\n",
       "      occurance_number_lowercased  occurance_number_propercased  \n",
       "4203                            4                             1  \n",
       "4814                            4                             1  \n",
       "4927                            4                             1  \n",
       "5079                           11                             1  \n",
       "5080                            9                             1  \n",
       "7579                            5                             1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_train[single_train['occurance_number_lowercased']>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approval of the minutes of the previous sitting: see minutes\n",
      "Approval of Minutes of previous sitting: see Minutes\n"
     ]
    }
   ],
   "source": [
    "for s in single_train[single_train['occurance_number_propercased']>1]['sentence']:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single words\n",
    "### trial\n",
    "* always only 1 target occuarnce for propercased\n",
    "* 6 sentences with 2 occurances for lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_trial['occurance_number_lowercased'] = single_trial.apply(lambda row:\n",
    "    row['sentence'].lower().count(row['token'].lower()), axis=1\n",
    ")\n",
    "\n",
    "single_trial['occurance_number_propercased'] = single_trial.apply(lambda row:\n",
    "    row['sentence'].count(row['token']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(single_trial['occurance_number_lowercased'].unique())\n",
    "print(single_trial['occurance_number_propercased'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "      <th>occurance_number_lowercased</th>\n",
       "      <th>occurance_number_propercased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3EFNPKWBMSO9IYBXCIW0X6IAX8E030</td>\n",
       "      <td>biomed</td>\n",
       "      <td>Lung development in Dhcr7-/- embryos at the ea...</td>\n",
       "      <td>Lung</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>379OL9DBSSESUVWY1Z8JGBFG9E19YR</td>\n",
       "      <td>biomed</td>\n",
       "      <td>Rod spherules establish an invaginating synaps...</td>\n",
       "      <td>Rod</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>3P0I4CQYVY7RCD54ON9DS4PPT5QOWO</td>\n",
       "      <td>europarl</td>\n",
       "      <td>We have simply confirmed, in accordance with o...</td>\n",
       "      <td>Rules</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>3TFJJUELSHP4R8AUKYBF9XFJ0LWC2J</td>\n",
       "      <td>europarl</td>\n",
       "      <td>Proposal for a Council Decision establishing f...</td>\n",
       "      <td>Justice</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>31GECDVA9JM3TSKUX9AFDA4LK3466H</td>\n",
       "      <td>europarl</td>\n",
       "      <td>The proposal to amend Regulation (EC) No 539/2...</td>\n",
       "      <td>EC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>3OQQD2WO8I6KPTSDG8L63AI6J4E3IL</td>\n",
       "      <td>europarl</td>\n",
       "      <td>the report by Mr Albertini, on behalf of the C...</td>\n",
       "      <td>EC</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id    corpus  \\\n",
       "171  3EFNPKWBMSO9IYBXCIW0X6IAX8E030    biomed   \n",
       "252  379OL9DBSSESUVWY1Z8JGBFG9E19YR    biomed   \n",
       "280  3P0I4CQYVY7RCD54ON9DS4PPT5QOWO  europarl   \n",
       "318  3TFJJUELSHP4R8AUKYBF9XFJ0LWC2J  europarl   \n",
       "417  31GECDVA9JM3TSKUX9AFDA4LK3466H  europarl   \n",
       "418  3OQQD2WO8I6KPTSDG8L63AI6J4E3IL  europarl   \n",
       "\n",
       "                                              sentence    token  complexity  \\\n",
       "171  Lung development in Dhcr7-/- embryos at the ea...     Lung    0.175000   \n",
       "252  Rod spherules establish an invaginating synaps...      Rod    0.400000   \n",
       "280  We have simply confirmed, in accordance with o...    Rules    0.178571   \n",
       "318  Proposal for a Council Decision establishing f...  Justice    0.203125   \n",
       "417  The proposal to amend Regulation (EC) No 539/2...       EC    0.500000   \n",
       "418  the report by Mr Albertini, on behalf of the C...       EC    0.605263   \n",
       "\n",
       "     occurance_number_lowercased  occurance_number_propercased  \n",
       "171                            2                             1  \n",
       "252                            2                             1  \n",
       "280                            2                             1  \n",
       "318                            2                             1  \n",
       "417                            2                             1  \n",
       "418                            2                             1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(single_trial[single_trial['occurance_number_lowercased']>1]))\n",
    "single_trial[single_trial['occurance_number_lowercased']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rod spherules establish an invaginating synapse with rod bipolar dendrites and axonal endings of horizontal cells.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_trial['sentence'][252]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE\n",
    "### train\n",
    "* 5 sentences with 2 occurances of target MWE if lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_train['occurance_number_lowercased'] = multi_train.apply(lambda row:\n",
    "    row['sentence'].lower().count(row['token'].lower()), axis=1\n",
    ")\n",
    "\n",
    "multi_train['occurance_number_propercased'] = multi_train.apply(lambda row:\n",
    "    row['sentence'].count(row['token']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(multi_train['occurance_number_lowercased'].unique())\n",
    "print(multi_train['occurance_number_propercased'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(multi_train[multi_train['occurance_number_lowercased']>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>complexity</th>\n",
       "      <th>occurance_number_lowercased</th>\n",
       "      <th>occurance_number_propercased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>3O4VWC1GEW6GK4CJYQ66FBX6WJ5J3F</td>\n",
       "      <td>europarl</td>\n",
       "      <td>Proposal for a Council Decision establishing f...</td>\n",
       "      <td>Fundamental Rights</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>3O4VWC1GEW6GK4CJYQ66FBX6WJ53JZ</td>\n",
       "      <td>europarl</td>\n",
       "      <td>- Madam President, the World Food Summit last ...</td>\n",
       "      <td>food security</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>30UZJB2POHC8D5XY9O2CE1E1EIA35D</td>\n",
       "      <td>europarl</td>\n",
       "      <td>Support for rural development by the European ...</td>\n",
       "      <td>rural development</td>\n",
       "      <td>0.329545</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>3AA88CN98P3CBRFP5WZ86KTWK90YKZ</td>\n",
       "      <td>europarl</td>\n",
       "      <td>Revision of the Treaties - Transitional measur...</td>\n",
       "      <td>transitional measures</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>3G5RUKN2EC3YIWSKUXZ8ZVH95VKN94</td>\n",
       "      <td>europarl</td>\n",
       "      <td>The next item is the report by Tanja Fajon, on...</td>\n",
       "      <td>Council regulation</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id    corpus  \\\n",
       "1110  3O4VWC1GEW6GK4CJYQ66FBX6WJ5J3F  europarl   \n",
       "1185  3O4VWC1GEW6GK4CJYQ66FBX6WJ53JZ  europarl   \n",
       "1236  30UZJB2POHC8D5XY9O2CE1E1EIA35D  europarl   \n",
       "1390  3AA88CN98P3CBRFP5WZ86KTWK90YKZ  europarl   \n",
       "1406  3G5RUKN2EC3YIWSKUXZ8ZVH95VKN94  europarl   \n",
       "\n",
       "                                               sentence  \\\n",
       "1110  Proposal for a Council Decision establishing f...   \n",
       "1185  - Madam President, the World Food Summit last ...   \n",
       "1236  Support for rural development by the European ...   \n",
       "1390  Revision of the Treaties - Transitional measur...   \n",
       "1406  The next item is the report by Tanja Fajon, on...   \n",
       "\n",
       "                      token  complexity  occurance_number_lowercased  \\\n",
       "1110     Fundamental Rights    0.273810                            2   \n",
       "1185          food security    0.315789                            2   \n",
       "1236      rural development    0.329545                            2   \n",
       "1390  transitional measures    0.444444                            2   \n",
       "1406     Council regulation    0.462500                            2   \n",
       "\n",
       "      occurance_number_propercased  \n",
       "1110                             1  \n",
       "1185                             1  \n",
       "1236                             1  \n",
       "1390                             1  \n",
       "1406                             1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_train[multi_train['occurance_number_lowercased']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Support for rural development by the European Agricultural Fund for Rural Development (EAFRD) ('"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_train['sentence'][1236]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE\n",
    "### trial\n",
    "only unique occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "multi_trial['occurance_number_lowercased'] = multi_trial.apply(lambda row:\n",
    "    row['sentence'].lower().count(row['token'].lower()), axis=1\n",
    ")\n",
    "\n",
    "multi_trial['occurance_number_propercased'] = multi_trial.apply(lambda row:\n",
    "    row['sentence'].count(row['token']), axis=1\n",
    ")\n",
    "\n",
    "print(multi_trial['occurance_number_lowercased'].unique())\n",
    "print(multi_trial['occurance_number_propercased'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "## are there any words not in BERT vocab? (are there words that are segmented futher?) (YES)\n",
    "\n",
    "There are lots of tokens that are not identical to vocabulary tokens of BERT. They are segmented further into subwords. That means there should be a representation based on subwords. Good news: it seems that there are no unks in those subwords. Lowercased model knows more tokens than the propercased model.\n",
    "\n",
    "**BAD NEWS:** BERT tokenizer tokenizes target word on its own differently than in context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voskobe1/.conda/envs/diplomchik/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_propercased = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenizer_lowercased = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOWERCASED\n",
      "There are 929 word types absent out of 3298\n",
      "There are 50 word types absent out of 212\n",
      "PROPER CASED\n",
      "There are 1310 word types absent out of 3487\n",
      "There are 73 word types absent out of 213\n"
     ]
    }
   ],
   "source": [
    "def check_vocab_singles(vocab, words, get_not_in_vocab=True, lowercased=True):\n",
    "    \"\"\"Checks for target words in a model's vocabulary\n",
    "    \n",
    "    Prints out the number of target words absent from the vocabulary.\n",
    "    \n",
    "    WHEN get_not_in_vocab=True, returns a list of words absent from the vocabulary\n",
    "    WHEN lowercased=True, the target words are lowercased to match the lowercased vocabulary given\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : iterable object of strings\n",
    "        a model's vocabulary (a set of all known tokens)\n",
    "    words : iterable object of strings\n",
    "        a set of target word tokens that we hope to find in the vocabulary\n",
    "    get_not_in_vocab : bool\n",
    "        indicates if a list of words absent from a vocabulary should be returned (True for yes)\n",
    "    lowercased : bool\n",
    "        indicates if vocab containes only lowercased words, so targets should be lowercased to match that\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    not_in_vocab : list of strings\n",
    "        a list of words absent from a vocabulary\n",
    "        \n",
    "    \"\"\"\n",
    "    if lowercased:\n",
    "        words = set([word.lower() for word in words])\n",
    "        not_in_vocab = [word for word in words if word not in vocab]\n",
    "    else:\n",
    "        not_in_vocab = [word for word in words if word not in vocab]\n",
    "    if len(not_in_vocab) == 0:\n",
    "        print(\"All word types are in the model's vocabulary\")\n",
    "    else:\n",
    "        print(\"There are\", len(not_in_vocab), 'word types absent out of', len(words))\n",
    "        if get_not_in_vocab:\n",
    "            return not_in_vocab\n",
    "\n",
    "print('LOWERCASED')\n",
    "not_in_vocab_train_lowercased = check_vocab_singles(tokenizer_lowercased.vocab, set(single_train['token']))\n",
    "not_in_vocab_trial_lowercased = check_vocab_singles(tokenizer_lowercased.vocab, set(single_trial['token']))\n",
    "print('PROPER CASED')\n",
    "not_in_vocab_train_propercased = check_vocab_singles(tokenizer_propercased.vocab, set(single_train['token']), lowercased=False)\n",
    "not_in_vocab_trial_propercased = check_vocab_singles(tokenizer_propercased.vocab, set(single_trial['token']), lowercased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOWERCASED\n",
      "There are 473 MWEs not segmented into just two words out of 1263\n",
      "There are 34 MWEs not segmented into just two words out of 76\n",
      "PROPERCASED\n",
      "There are 576 MWEs not segmented into just two words out of 1270\n",
      "There are 40 MWEs not segmented into just two words out of 76\n"
     ]
    }
   ],
   "source": [
    "def check_vocab_mwe(tokenizer, mwes, get_not_in_vocab=True, lowercased=True):\n",
    "    \"\"\"Checks for both words in a pair being in a model's vocabulary\n",
    "    \n",
    "    Prints out the number of target pairs, where a pair tokenized by a given tokenizer doesn't match this pair separated by whitespace\n",
    "    \n",
    "    WHEN get_not_in_vocab=True, returns a list of target pairs where one or both words are absent from the vocabulary\n",
    "    WHEN lowercased=True, the target pairs are lowercased to match the lowercased vocabulary used by a tokenizer\n",
    "    \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : a tokenizer object\n",
    "        takes a string and returns a list of string tokens\n",
    "    mwes : iterable object of strings\n",
    "        target word pairs that we hope to find in the vocabulary \n",
    "    get_not_in_vocab : bool\n",
    "        indicates if a list of target pairs where one or both words are absent from the vocabulary should be returned (True for yes)\n",
    "    lowercased : bool\n",
    "        indicates if vocab containes only lowercased words, so targets should be lowercased to match that\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    not_in_vocab : list of strings\n",
    "        a list of target pairs where one or both words are absent from the vocabulary\n",
    "        \n",
    "    \"\"\"\n",
    "    if lowercased:\n",
    "        mwes = [word_pair.lower() for word_pair in mwes]\n",
    "        not_in_vocab = set([word_pair for word_pair in mwes if tokenizer.tokenize(word_pair)!=word_pair.split(\" \")])\n",
    "    else:\n",
    "        not_in_vocab = set([word_pair for word_pair in mwes if tokenizer.tokenize(word_pair)!=word_pair.split(\" \")])\n",
    "    \n",
    "    if len(not_in_vocab) == 0:\n",
    "        print(\"All mwe tokens are in the model's vocabulary\")\n",
    "    else:\n",
    "        print(\"There are\", len(not_in_vocab), 'MWEs not segmented into just two words out of', len(set(mwes)))\n",
    "        if get_not_in_vocab:\n",
    "            return not_in_vocab\n",
    "\n",
    "print('LOWERCASED')\n",
    "not_in_vocab_train_mwe_lowercased = check_vocab_mwe(tokenizer_lowercased, multi_train['token'])\n",
    "not_in_vocab_trial_mwe_lowercased = check_vocab_mwe(tokenizer_lowercased, multi_trial['token'])\n",
    "print('PROPERCASED')\n",
    "not_in_vocab_train_mwe_propercased = check_vocab_mwe(tokenizer_propercased, multi_train['token'], lowercased=False)\n",
    "not_in_vocab_trial_mwe_propercased = check_vocab_mwe(tokenizer_propercased, multi_trial['token'], lowercased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "## are there any oov subwords in oov words? (NO)\n",
    "Good news: it seems that there are no unks in subwords of targets\n",
    "\n",
    "There are a couple of UNKs in sentences for single targets. I don't really care about those for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All subwords are in vocab\n",
      "All subwords are in vocab\n",
      "All subwords are in vocab\n",
      "All subwords are in vocab\n"
     ]
    }
   ],
   "source": [
    "def check_oovs(not_in_vocab, tokenizer, unk_token = '[UNK]'):\n",
    "    \"\"\"Checks if words missing from a vocabulary contain UNK subwords after tokenization\n",
    "    \n",
    "    Prints out the number of UNK tokens found after tokenization of target words or MWEs missing from a vocabulary.\n",
    "    Can also be used to check if sentences contain any UNKs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    not_in_vocab : iterable object of strings\n",
    "        target word pairs that we hope to find in the vocabulary \n",
    "    tokenizer : a tokenizer object\n",
    "        takes a string and returns a list of string tokens\n",
    "    unk_token : str\n",
    "        a token used to mark OOV string by a tokenizer, default to BERT unk\n",
    "        \n",
    "    \"\"\"\n",
    "    contains_unk_subs = 0\n",
    "    for string in not_in_vocab:\n",
    "        subwords = tokenizer.tokenize(string)\n",
    "        if unk_token in subwords:\n",
    "            contains_unk_subs+=1\n",
    "    if contains_unk_subs>0:\n",
    "        print('There are', contains_unk_subs, 'UNKs')\n",
    "    else:\n",
    "        print('All subwords are in vocab')\n",
    "\n",
    "check_oovs(not_in_vocab_train_propercased, tokenizer_propercased)\n",
    "check_oovs(not_in_vocab_trial_propercased, tokenizer_propercased)\n",
    "check_oovs(not_in_vocab_train_lowercased, tokenizer_lowercased)\n",
    "check_oovs(not_in_vocab_trial_lowercased, tokenizer_lowercased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All subwords are in vocab\n",
      "All subwords are in vocab\n",
      "All subwords are in vocab\n",
      "All subwords are in vocab\n"
     ]
    }
   ],
   "source": [
    "check_oovs(not_in_vocab_train_mwe_lowercased, tokenizer_lowercased)\n",
    "check_oovs(not_in_vocab_trial_mwe_lowercased, tokenizer_lowercased)\n",
    "check_oovs(not_in_vocab_train_mwe_propercased, tokenizer_propercased)\n",
    "check_oovs(not_in_vocab_trial_mwe_propercased, tokenizer_propercased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinlgles train lowercased\n",
      "There are 1 UNKs\n",
      "Sinlgles train propercased\n",
      "There are 3 UNKs\n",
      "Sinlgles trial lowercased\n",
      "All subwords are in vocab\n",
      "Sinlgles trial propercased\n",
      "All subwords are in vocab\n",
      "MWE train lowercased\n",
      "All subwords are in vocab\n",
      "MWE train propercased\n",
      "All subwords are in vocab\n",
      "MWE trial lowercased\n",
      "All subwords are in vocab\n",
      "MWE trial propercased\n",
      "All subwords are in vocab\n"
     ]
    }
   ],
   "source": [
    "print('Sinlgles train lowercased')\n",
    "check_oovs(single_train['sentence'], tokenizer_lowercased)\n",
    "print('Sinlgles train propercased')\n",
    "check_oovs(single_train['sentence'], tokenizer_propercased)\n",
    "print('Sinlgles trial lowercased')\n",
    "check_oovs(single_trial['sentence'], tokenizer_lowercased)\n",
    "print('Sinlgles trial propercased')\n",
    "check_oovs(single_trial['sentence'], tokenizer_propercased)\n",
    "\n",
    "print('MWE train lowercased')\n",
    "check_oovs(multi_train['sentence'], tokenizer_lowercased)\n",
    "print('MWE train propercased')\n",
    "check_oovs(multi_train['sentence'], tokenizer_propercased)\n",
    "print('MWE trial lowercased')\n",
    "check_oovs(multi_trial['sentence'], tokenizer_lowercased)\n",
    "print('MWE trial propercased')\n",
    "check_oovs(multi_trial['sentence'], tokenizer_propercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "How to deal with the fact that a target word can be tokenized by BERT differently when its alone and in some context? E.g:  How to make sure that we find the right BERT tokens for targets?\n",
    "\n",
    "for a single word:\n",
    "* 1. check if a target word token is in sentence tokens (and only once)\n",
    "* 2. if not, check if a tokenized target word is \n",
    "  * A tokenizable into subparts\n",
    "  * B in sentence tokens (once)\n",
    "* 3. if not, check if a target string is a substring of any sentence tokens (and only once)\n",
    "* 4. give out what is completely missing\n",
    "\n",
    "\n",
    "for a MWE:\n",
    "* 1. check if a tokenized pair is in sentence tokens (and only once)\n",
    "* 2. give out what is completely missing\n",
    "\n",
    "SUMMARY:\n",
    "So, for the propercased model, everything is found, in the end. Minutes are still the only multiple occurance there for singles df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokenized_sentence_singles(tokenizer, df, get_not_in_vocab=True, lowercased=True):\n",
    "    \"\"\"Checks for sentence tokens that represent target words\n",
    "    \n",
    "    Prints out:\n",
    "    The number of targets that were not found at all\n",
    "    The number of targets that are not represented as a standalone whole in a tokenized sentence ('##' appended counted here)\n",
    "    The number of targets that were tokenized into subwords and those subwords match a tokenized sentence sublist\n",
    "    The number of targets that were not segmented further, but instead were undersegmented in a sentence, \n",
    "        and thus were a substring of some sentence token.\n",
    "    The number of times a target was found represented several times in a tokenized sentence.\n",
    "    \n",
    "    WHEN get_not_in_vocab=True, returns a tuple of lists of targets that constitues the statistics described above\n",
    "    WHEN lowercased=True, the target words are lowercased to match the lowercased vocabulary given\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : a tokenizer object\n",
    "        takes a string and returns a list of string tokens\n",
    "    df : pandas dataframe\n",
    "        a dataframe containing a sentence (df.sentene) and target words (df.token) at the same row\n",
    "    get_not_in_vocab : bool\n",
    "        indicates if a tuple of lists of targets that constitues the printed statistics should be returned (True for yes)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    completely_missing : a list of tuples (str, list of strings)\n",
    "        a list of tuples for targets that were not found at all and their tokenized sentences\n",
    "    not_in_vocab : a list of strings\n",
    "        target words that are not represented as a standalone whole in a tokenized sentence \n",
    "    subwords_in_vocab : a list of tuples (str, list of strings)\n",
    "        a list of tuples for targets (and their tokenized sentences) that were tokenized into subwords \n",
    "        and those subwords match a tokenized sentence sublist\n",
    "    partly_in_vocab : a list of tuples (str, list of strings)\n",
    "        a list of tuples for targets (and their tokenized sentences) that were not segmented further, \n",
    "        but instead were undersegmented in a sentence, and thus were a substring of some sentence token \n",
    "    multiple_occurances : a list of tuples (str, list of strings)\n",
    "        a list of tuples for targets (and their tokenized sentences) that were matched several times in a tokenized sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    not_in_vocab = []\n",
    "    subwords_in_vocab = []\n",
    "    partly_in_vocab = []\n",
    "    completely_missing = []\n",
    "    multiple_occurances = []\n",
    "    \n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tokenized_sentence = tokenizer.tokenize(row.sentence)\n",
    "        if lowercased:\n",
    "            target_word = row.token.lower()\n",
    "        else:\n",
    "            target_word = row.token\n",
    "        \n",
    "        ##########################################\n",
    "        num_target = tokenized_sentence.count(target_word)\n",
    "        \n",
    "        ##############\n",
    "        # CHECK 1\n",
    "        ##############\n",
    "        if num_target > 1:\n",
    "            multiple_occurances.append((target_word, tokenized_sentence))\n",
    "        \n",
    "        if num_target == 0:\n",
    "            not_in_vocab.append(target_word)\n",
    "            ##############\n",
    "            # CHECK 2\n",
    "            ##############\n",
    "            # tokenize target and look for a subword list match\n",
    "            tokenized_word = tokenizer.tokenize(target_word)\n",
    "            # 2.A\n",
    "            if len(tokenized_word) > 1:\n",
    "                parts_len = len(tokenized_word)\n",
    "                sentence_len = len(tokenized_sentence)\n",
    "                \n",
    "                # 2.B\n",
    "                current_match = 0\n",
    "                for i in range(sentence_len-parts_len+1):\n",
    "                    if tokenized_sentence[i:i+parts_len] == tokenized_word:\n",
    "                        current_match += 1 \n",
    "                if current_match > 1:\n",
    "                    multiple_occurances.append((target_word, tokenized_sentence))\n",
    "                    subwords_in_vocab.append((target_word, tokenized_sentence))\n",
    "                if current_match == 1:\n",
    "                    subwords_in_vocab.append((target_word, tokenized_sentence))\n",
    "                # if nothing found\n",
    "                if current_match == 0:\n",
    "                    completely_missing.append((target_word, tokenized_sentence))\n",
    "                        \n",
    "            ##############\n",
    "            # CHECK 3\n",
    "            ##############\n",
    "            # look for a token in a sentence that has a target as its part\n",
    "            else:\n",
    "                partly_tokenized = [word for word in tokenized_sentence if target_word in word]\n",
    "                \n",
    "                if len(partly_tokenized)>1:\n",
    "                    multiple_occurances.append((target_word, tokenized_sentence))\n",
    "                \n",
    "                if len(partly_tokenized)==1:\n",
    "                    partly_in_vocab.append((target_word, tokenized_sentence))\n",
    "                if len(partly_tokenized)==0:\n",
    "                    completely_missing.append((target_word, tokenized_sentence))\n",
    "        \n",
    "    print('Completely missing', len(completely_missing))\n",
    "    print('Initially not found', len(not_in_vocab), 'out of', len(df))\n",
    "    print('Segmented and found', len(subwords_in_vocab))\n",
    "    print('Part of a sentence token', len(partly_in_vocab))\n",
    "    print('Multiple occurances', len(multiple_occurances))\n",
    "    \n",
    "    return completely_missing, not_in_vocab, subwords_in_vocab, partly_in_vocab, multiple_occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOWERCASED\n",
      "------------\n",
      "train\n",
      "------------\n",
      "Completely missing 0\n",
      "Initially not found 1850 out of 7662\n",
      "Segmented and found 1849\n",
      "Part of a sentence token 0\n",
      "Multiple occurances 49\n",
      "------------\n",
      "trial\n",
      "------------\n",
      "Completely missing 0\n",
      "Initially not found 82 out of 421\n",
      "Segmented and found 82\n",
      "Part of a sentence token 0\n",
      "Multiple occurances 4\n",
      "--------------\n",
      "CASED\n",
      "------------\n",
      "train\n",
      "------------\n",
      "Completely missing 0\n",
      "Initially not found 2470 out of 7662\n",
      "Segmented and found 2469\n",
      "Part of a sentence token 1\n",
      "Multiple occurances 2\n",
      "------------\n",
      "trial\n",
      "------------\n",
      "Completely missing 0\n",
      "Initially not found 135 out of 421\n",
      "Segmented and found 135\n",
      "Part of a sentence token 0\n",
      "Multiple occurances 0\n"
     ]
    }
   ],
   "source": [
    "print('LOWERCASED')\n",
    "print('------------')\n",
    "print('train')\n",
    "print('------------')\n",
    "results_lowercased_single_train = check_tokenized_sentence_singles(tokenizer_lowercased, single_train)\n",
    "print('------------')\n",
    "print('trial')\n",
    "print('------------')\n",
    "results_lowercased_single_trial = check_tokenized_sentence_singles(tokenizer_lowercased, single_trial)\n",
    "print('--------------')\n",
    "print('CASED')\n",
    "print('------------')\n",
    "print('train')\n",
    "print('------------')\n",
    "results_propercased_single_train = check_tokenized_sentence_singles(tokenizer_propercased, single_train, lowercased=False)\n",
    "print('------------')\n",
    "print('trial')\n",
    "print('------------')\n",
    "results_propercased_single_trial = check_tokenized_sentence_singles(tokenizer_propercased, single_trial, lowercased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokenized_sentence_mwes(tokenizer, df, get_not_in_vocab=True):\n",
    "    \"\"\"Checks for sentence tokens that represent parts of target pairs\n",
    "    \n",
    "    Prints out:\n",
    "    The number of target pairs where their subword list was not found as a sublist of sentence tokens\n",
    "    The number of times a target pair was found represented several times in a tokenized sentence\n",
    "\n",
    "    WHEN get_not_in_vocab=True, returns a tuple of lists of targets that constitues the statistics described above\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : a tokenizer object\n",
    "        takes a string and returns a list of string tokens\n",
    "    df : pandas dataframe\n",
    "        a dataframe containing a sentence (df.sentene) and its target pairs (df.token) at the same row\n",
    "    get_not_in_vocab : bool\n",
    "        indicates if a tuple of lists of targets that constitues the printed statistics should be returned (True for yes)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    completely_missing : a list of tuples (str, list of strings)\n",
    "        a list of tuples for tokenized target pairs (and their tokenized sentences) that were not found at all in a sentence\n",
    "    multiple_occurances : a list of tuples (str, list of strings)\n",
    "        a list of tuples for tokenixed target pairs (and their tokenized sentences) that were matched several times in a tokenized sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    completely_missing = []\n",
    "    multiple_occurances = []\n",
    "    \n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tokenized_sentence = tokenizer.tokenize(row.sentence)\n",
    "        tokenized_target_pair = tokenizer.tokenize(row.token)\n",
    "        \n",
    "        ##############\n",
    "        # CHECK 1\n",
    "        ##############\n",
    "        parts_len = len(tokenized_target_pair)\n",
    "        sentence_len = len(tokenized_sentence)\n",
    "\n",
    "        current_match = 0\n",
    "        for i in range(sentence_len-parts_len+1):\n",
    "            if tokenized_sentence[i:i+parts_len] == tokenized_target_pair:\n",
    "                current_match += 1 \n",
    "        if current_match > 1:\n",
    "            multiple_occurances.append((tokenized_target_pair, tokenized_sentence))\n",
    "        # if nothing found\n",
    "        if current_match == 0:\n",
    "            completely_missing.append((tokenized_target_pair, tokenized_sentence))\n",
    "\n",
    "        \n",
    "    print('Completely missing', len(completely_missing), 'out of', len(df))\n",
    "    print('Multiple occurances', len(multiple_occurances))\n",
    "    \n",
    "    return completely_missing, multiple_occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOWERCASED\n",
      "------------\n",
      "train\n",
      "------------\n",
      "Completely missing 0 out of 1517\n",
      "Multiple occurances 5\n",
      "------------\n",
      "trial\n",
      "------------\n",
      "Completely missing 0 out of 99\n",
      "Multiple occurances 0\n",
      "--------------\n",
      "CASED\n",
      "------------\n",
      "train\n",
      "------------\n",
      "Completely missing 0 out of 1517\n",
      "Multiple occurances 0\n",
      "------------\n",
      "trial\n",
      "------------\n",
      "Completely missing 0 out of 99\n",
      "Multiple occurances 0\n"
     ]
    }
   ],
   "source": [
    "print('LOWERCASED')\n",
    "print('------------')\n",
    "print('train')\n",
    "print('------------')\n",
    "results_lowercased_mwe_train = check_tokenized_sentence_mwes(tokenizer_lowercased, multi_train)\n",
    "print('------------')\n",
    "print('trial')\n",
    "print('------------')\n",
    "results_lowercased_mwe_trial = check_tokenized_sentence_mwes(tokenizer_lowercased, multi_trial)\n",
    "print('--------------')\n",
    "print('CASED')\n",
    "print('------------')\n",
    "print('train')\n",
    "print('------------')\n",
    "results_propercased_mwe_train = check_tokenized_sentence_mwes(tokenizer_propercased, multi_train)\n",
    "print('------------')\n",
    "print('trial')\n",
    "print('------------')\n",
    "results_propercased_mwe_trial = check_tokenized_sentence_mwes(tokenizer_propercased, multi_trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('diplomchik': conda)",
   "language": "python",
   "name": "python36664bitdiplomchikcondac9dda3a2509d43ad9b71af24f3396da7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
