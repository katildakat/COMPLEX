{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT FEATURES\n",
    "## embedding based\n",
    "* ✓ get a sentence embedding (easy with any model)\n",
    "* ✓ get a single word embedding (how to ensure for uncased model that the right word index is given? use cased model for now)\n",
    "* ✓ get a MWE embedding (same as for single words that are tokenized into several subwords)\n",
    "\n",
    "* get a top masked single word embedding\n",
    "* get a masked MWE embedding (one mask for now)\n",
    "* get a sentence embedding with masked target word\n",
    "* get a distance from masked embedding to a target embedding\n",
    "* ✓ get a score for a masked target (average for parts of a tokenized target)\n",
    "\n",
    "## tokenization based\n",
    "* ✓ get a length of a tokenized target (/2 for mwe)\n",
    "* ✓ get a length of tokenized target in characters ((len-1)/2 for mwe)\n",
    "* ✓ get a sentence length in tokens\n",
    "* ✓ get an average sentence length (average number of tokens per word)\n",
    "\n",
    "# OTHER FEATURES\n",
    "* ✓ number WordNet senses for a word\n",
    "* ✓ log frequency in general english\n",
    "* frequency in a specific corpus vs general (rank comparison)\n",
    "* ✓ number of morphs\n",
    "* ✓ frequency of morphs\n",
    "\n",
    "\n",
    "TO DO:\n",
    "✓ deal with test 38LRF35D5LWPYKNDAPAKMD6HD1M3UI (lowercase the target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Data loading](#data_load) \n",
    "* [BERT loading](#bert_load)\n",
    "    * [BERT cheat sheet](#bert_cheat)\n",
    "* [1: get token indices](#1)\n",
    "* [2: get non-mask features](#2)\n",
    "    * [3: save non-mask features](#3)\n",
    "* [4: get MASK-related features](#4)\n",
    "* [5: get WordNet features](#5)\n",
    "* [6: get word frequency features](#6)\n",
    "* [7: get Morfessor features](#7)\n",
    "* [8: save non-embedding features](#8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voskobe1/.conda/envs/diplomchik/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, AutoModelWithLMHead\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data <a class=\"anchor\" id=\"data_load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file_name):\n",
    "    df = pd.read_csv(file_name, '\\t', quoting=3, na_filter=False)\n",
    "    if 'subcorpus' in df.columns:\n",
    "        df = df.rename(columns={'subcorpus':'corpus'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = data_loader('data/train/lcp_single_train.tsv')\n",
    "multi_train = data_loader('data/train/lcp_multi_train.tsv')\n",
    "\n",
    "single_trial = data_loader('data/trial/lcp_single_trial.tsv')\n",
    "multi_trial = data_loader('data/trial/lcp_multi_trial.tsv')\n",
    "\n",
    "single_test = data_loader('data/test/lcp_single_test_labels.tsv')\n",
    "multi_test = data_loader('data/test/lcp_multi_test_labels.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BERT models  <a class=\"anchor\" id=\"bert_load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_propercased = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model_propercased = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "config = model_propercased.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT cheat sheet <a class=\"anchor\" id=\"bert_cheat\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n",
      "['[CLS]', 'sea', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'This is A sentence!'\n",
    "sentence_token_ids = tokenizer_propercased('sea.')['input_ids']\n",
    "sentence_token_ids_tensor = tokenizer_propercased('sea.', padding=True, truncation=True, return_tensors=\"pt\" )['input_ids']\n",
    "sentence_tokens = [tokenizer_propercased.ids_to_tokens[id] for id in sentence_token_ids]\n",
    "print(tokenizer_propercased.mask_token)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 get the indices of the subwords for a target word/word pair <a class=\"anchor\" id=\"1\"></a>\n",
    "1. tokenize a sentence + tokenize a target\n",
    "2. check if target is one subword long\n",
    "* **case 1**: word is tokenized as a whole\n",
    "    3. check that a word is in sentence tokens and get its index (indices for multiple occurances) in a sentence\n",
    "    4. otherwise get a sentence token that contains the target as its part\n",
    "* **case 2**: word is tokenized into subwords\n",
    "    3. get indices of subwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(tokenized_sentence, tokenized_target):\n",
    "    \"\"\"Returns positions of target tokens in a tokenized sentence\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenized_sentence : iterable object\n",
    "        an iterable object with sentence tokens including special tokens\n",
    "    tokenized_target : iterable object\n",
    "        an iterable object with target tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target_inds : indices of target parts in a tokenized sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    target_inds=[]\n",
    "    parts_len = len(tokenized_target)\n",
    "    \n",
    "    # if target is tokenized into several subwords\n",
    "    if parts_len > 1:\n",
    "        sentence_len = len(tokenized_sentence)\n",
    "        for i in range(sentence_len-parts_len+1):\n",
    "            if tokenized_sentence[i:i+parts_len] == tokenized_target:\n",
    "                target_inds += [j for j in range(i,i+parts_len)]\n",
    "    \n",
    "    # if target is left as a whole\n",
    "    if parts_len == 1:\n",
    "        target = tokenized_target[0]\n",
    "        if target in tokenized_sentence:\n",
    "            # get indices of all target occurances\n",
    "            target_inds += [i for i,val in enumerate(tokenized_sentence) if val==target]\n",
    "        else:\n",
    "            # get indices of all tokens that contain a target\n",
    "            target_inds += [i for i,val in enumerate(tokenized_sentence) if target in val]\n",
    "    \n",
    "    return target_inds            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4]\n",
      "[0, 1]\n",
      "[0, 1, 4, 5]\n",
      "[4, 5]\n",
      "[0, 3]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(get_index([1,2,3,4,1],[1]))\n",
    "print(get_index([1,2,3,4,1],[1,2]))\n",
    "print(get_index([1,2,3,4,1,2],[1,2]))\n",
    "print(get_index([1,3,2,4,1,2],[1,2]))\n",
    "print(get_index(['ab','c','d','ac'],['a']))\n",
    "print(get_index(['ab','c','d','a'],['a']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 getting features (not mask-related) <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "1. ✓ tokenize a sentence and a target word\n",
    "2. ✓ get a sentence len in tokens (num of tokens - 2 special tokens)\n",
    "3. ✓ get an number of tokens per word in a sentence (num of tokens - 2 special tokens/(len(sentence.split())) \n",
    "4. ✓ get a number of tokens in a target (/2 for mwe)\n",
    "5. ✓ get a list of target indices\n",
    "6. ✓ put a sentence into a model\n",
    "7. ✓ get a sentence embedding\n",
    "8. ✓ get an average embedding for target parts\n",
    "\n",
    "**TO DO**:\n",
    "1. transform to batch-applicable\n",
    "2. 8 is not dealing with multiple occurances ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sentence, target_word, model, tokenizer, print_out=False):\n",
    "    \"\"\" \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str   \n",
    "    target_word : str\n",
    "    model :\n",
    "    tokenizer : \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : dict\n",
    "        sentence_embedding :\n",
    "        target_embedding : \n",
    "        sentence_len : \n",
    "        tokens_per_word :\n",
    "            an average token per word len in a sentence\n",
    "        target_len : \n",
    "        target_len_chars :\n",
    "    \"\"\"\n",
    "    # 1.1 tokenize a sentence\n",
    "    sentence_len_words = len(sentence.split())\n",
    "    sentence_token_ids = tokenizer(sentence)['input_ids']\n",
    "    sentence_tokens = tokenizer.convert_ids_to_tokens(sentence_token_ids)\n",
    "    \n",
    "    # 1.2 tokenize a target word\n",
    "    target_tokens = tokenizer.tokenize(target_word)\n",
    "    \n",
    "    # 2. get a sentence len in tokens\n",
    "    sentence_len = len(sentence_tokens) - 2\n",
    "    # 3. get an average word len in tokens\n",
    "    tokens_per_word = (len(sentence_tokens)-2)/sentence_len_words\n",
    "    # 4. get target len in tokens and characters\n",
    "    if \" \" in target_word:\n",
    "        target_len = len(target_tokens)/2\n",
    "        target_len_chars = (len(target_word)-1)/2\n",
    "    else:\n",
    "        target_len = len(target_tokens)\n",
    "        target_len_chars = len(target_word)\n",
    "    \n",
    "    # 5. get target parts indices\n",
    "    target_parts_ids = get_index(sentence_tokens, target_tokens)\n",
    "    \n",
    "    if print_out:\n",
    "        print(target_tokens)\n",
    "        print(sentence_tokens)\n",
    "        print(target_parts_ids)    \n",
    "    \n",
    "    # 6. put a sentence into a model\n",
    "    sentence_ids_tensor = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\" )['input_ids']\n",
    "    model_output = model(sentence_ids_tensor)[0] # batch x num indices x 768\n",
    "    \n",
    "    # 7. get a sentence embedding\n",
    "    sentence_embedding = model_output[0][0]\n",
    "    \n",
    "    # 8. get a target embedding\n",
    "    if len(target_tokens)==1:\n",
    "        # take only the first occasion\n",
    "        target_embedding = model_output[0][target_parts_ids[0]]\n",
    "    else:\n",
    "        #target_embedding = model_output[0][target_parts_ids[0]:target_parts_ids[-1]].mean(dim=0)\n",
    "        if len(target_parts_ids)==len(target_tokens):\n",
    "            target_embedding = model_output[0][target_parts_ids[0]:target_parts_ids[-1]].mean(dim=0)\n",
    "        else:\n",
    "            print(\"MULTIPLES!!!\", target_tokens)\n",
    "            # take only the first occasion\n",
    "            target_embedding = model_output[0][target_parts_ids[0]:target_parts_ids[len(target_tokens)]].mean(dim=0)\n",
    "            # take mean of everything\n",
    "            #target_embedding = model_output[0][target_parts_ids].mean(dim=0)\n",
    "            \n",
    "    features = {}\n",
    "    features['sentence_embedding'] = sentence_embedding\n",
    "    features['target_embedding'] = target_embedding\n",
    "    features['sentence_len'] = sentence_len\n",
    "    features['tokens_per_word'] = tokens_per_word\n",
    "    features['target_len'] = target_len\n",
    "    features['target_len_chars'] = target_len_chars\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sea', '##horse']\n",
      "['[CLS]', 'this', 'is', 'a', 'sea', '##horse', '.', 'hello', 'sea', '##horse', '[SEP]']\n",
      "[4, 5, 8, 9]\n",
      "MULTIPLES!!! ['sea', '##horse']\n",
      "--------\n",
      "9\n",
      "1.5\n",
      "2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sentence = 'this is a seahorse. hello seahorse'\n",
    "target_word = 'seahorse'\n",
    "\n",
    "features = get_features(sentence, target_word, model_propercased, tokenizer_propercased, True)\n",
    "print('--------')\n",
    "print(features['sentence_len'])\n",
    "print(features['tokens_per_word'])\n",
    "print(features['target_len'])\n",
    "print(features['target_len_chars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Save features <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "To not go over loading-predicting process again and again, it is easier to extract the features once.\n",
    "**TO DO**:\n",
    "1. transform to batch-applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_features(df, model, tokenizer):\n",
    "    \"\"\" Adds extracted features into a dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "    model\n",
    "    tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences=[]\n",
    "    targets=[]\n",
    "    sentence_lens=[]\n",
    "    tokens_per_words=[]\n",
    "    target_lens=[]\n",
    "    target_lens_chars=[]\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        if row.id =='38LRF35D5LWPYKNDAPAKMD6HD1M3UI':\n",
    "            features= get_features(row.sentence, row.token.lower(), model, tokenizer)\n",
    "        else:\n",
    "            features= get_features(row.sentence, row.token, model, tokenizer)\n",
    "        \n",
    "        sentences.append(features['sentence_embedding'].detach().numpy())\n",
    "        targets.append(features['target_embedding'].detach().numpy())\n",
    "        sentence_lens.append(features['sentence_len'])\n",
    "        target_lens.append(features['target_len'])\n",
    "        tokens_per_words.append(features['tokens_per_word'])\n",
    "        target_lens_chars.append(features['target_len_chars'])\n",
    "        \n",
    "    df['sentence_embedding'] = sentences\n",
    "    df['target_embedding'] = targets\n",
    "    df['sentence_len'] = sentence_lens\n",
    "    df['tokens_per_word'] = tokens_per_words\n",
    "    df['target_len'] = target_lens\n",
    "    df['target_len_chars'] = target_lens_chars\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving numpy arrays for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = write_features(single_train, model_propercased, tokenizer_propercased)\n",
    "single_trial = write_features(single_trial, model_propercased, tokenizer_propercased)\n",
    "single_test = write_features(single_test, model_propercased, tokenizer_propercased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_train = write_features(multi_train, model_propercased, tokenizer_propercased)\n",
    "multi_trial = write_features(multi_trial, model_propercased, tokenizer_propercased)\n",
    "multi_test = write_features(multi_test, model_propercased, tokenizer_propercased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_csv(file_name, df_embedding):\n",
    "    \n",
    "    numpy_embed = np.array(df_embedding.tolist())\n",
    "    np.savetxt(file_name, numpy_embed, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(\"features/single_target_embedding_train.csv\", single_train.target_embedding)\n",
    "embeddings_to_csv(\"features/single_target_embedding_trial.csv\", single_trial.target_embedding)\n",
    "embeddings_to_csv(\"features/single_target_embedding_test.csv\", single_test.target_embedding)\n",
    "\n",
    "embeddings_to_csv(\"features/single_sentence_embedding_train.csv\", single_train.sentence_embedding)\n",
    "embeddings_to_csv(\"features/single_sentence_embedding_trial.csv\", single_trial.sentence_embedding)\n",
    "embeddings_to_csv(\"features/single_sentence_embedding_test.csv\", single_test.sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(\"features/mwe_target_embedding_train.csv\", multi_train.target_embedding)\n",
    "embeddings_to_csv(\"features/mwe_target_embedding_trial.csv\", multi_trial.target_embedding)\n",
    "embeddings_to_csv(\"features/mwe_target_embedding_test.csv\", multi_test.target_embedding)\n",
    "\n",
    "embeddings_to_csv(\"features/mwe_sentence_embedding_train.csv\", multi_train.sentence_embedding)\n",
    "embeddings_to_csv(\"features/mwe_sentence_embedding_trial.csv\", multi_trial.sentence_embedding)\n",
    "embeddings_to_csv(\"features/mwe_sentence_embedding_test.csv\", multi_test.sentence_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 getting MASK-related features <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "* how to deal with thing in need of multiple masks? (only first occurance for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voskobe1/.conda/envs/diplomchik/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_masked_lm = AutoModelWithLMHead.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For single targets\n",
    "1. ✓ substitute target occurances for a mask in a sentence\n",
    "2. ✓ get sentence tokens ids\n",
    "3. ✓ get mask id (first_occurance if multiple)\n",
    "4. ✓ get logits for a masked token\n",
    "5. ✓ get max prob for masked token prediction\n",
    "6. ✓ get predicted token\n",
    "7. ✓ see if token was predicted right (impossible for subword tokenized singles)\n",
    "\n",
    "\n",
    "what if keeping later occurance for multiple unmasked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_features_singles(sentence, token, model_lm, tokenizer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence\n",
    "    token\n",
    "    model_lm\n",
    "    tokenizer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features: dict\n",
    "        predicted_prob : probability of a word that was precicted by a model\n",
    "        predicted_word : the word that was predicted by a model\n",
    "        target_prob : probability of a target word to be predicted (average for parts)\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # substitute target occurances for a mask\n",
    "    sentence = sentence.replace(token, tokenizer.mask_token)\n",
    "    # sentence = sentence.replace(token, tokenizer.mask_token, 1)\n",
    "    \n",
    "    # get sentence tokens ids\n",
    "    sentence_tensor = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    \n",
    "    # get mask index (first_occurance)\n",
    "    mask_token_index = torch.where(sentence_tensor == tokenizer.mask_token_id)[1][0].reshape(1)\n",
    "\n",
    "    # prediction for everything\n",
    "    logits = model_lm(sentence_tensor).logits\n",
    "    \n",
    "    # prediction for a masked token\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "    # probs of masked tokens\n",
    "    probs = torch.nn.functional.log_softmax(mask_token_logits, dim=1)\n",
    "    #probs = torch.nn.functional.softmax(mask_token_logits, dim=1)\n",
    "    predicted_prob = probs.max().item()\n",
    "    \n",
    "    # prediction as a word\n",
    "    top_1_tokens = torch.topk(mask_token_logits, 1, dim=1).indices\n",
    "    predicted_word = tokenizer.convert_ids_to_tokens(top_1_tokens)[0]\n",
    "    \n",
    "    # check if correct\n",
    "    tokenized_target = tokenizer.tokenize(token)\n",
    "    target_ids = tokenizer.convert_tokens_to_ids(tokenized_target)\n",
    "    target_logit = []\n",
    "    target_prob = []\n",
    "    for i in target_ids:\n",
    "        target_prob.append(probs[0][i].item())\n",
    "    target_prob = sum(target_prob)/len(target_prob)\n",
    "    \n",
    "    features['predicted_prob'] = predicted_prob\n",
    "    features['predicted_word'] = predicted_word\n",
    "    features['target_prob'] = target_prob\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "-3.44461989402771\n",
      "mistake\n",
      "-9.556841850280762\n"
     ]
    }
   ],
   "source": [
    "sentence = 'this is a sea. hello sea'\n",
    "target_word = 'sea'\n",
    "\n",
    "features = get_mask_features_singles(sentence, target_word, model_masked_lm, tokenizer_propercased)\n",
    "print('--------')\n",
    "print(features['predicted_prob'])\n",
    "print(features['predicted_word'])\n",
    "print(features['target_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mask_features(df, model_lm, tokenizer):\n",
    "    \n",
    "    best_probs=[]\n",
    "    predicted_words=[]\n",
    "    target_probs=[]\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row.id =='38LRF35D5LWPYKNDAPAKMD6HD1M3UI':\n",
    "            features= get_mask_features_singles(row.sentence, row.token.lower(), model_lm, tokenizer)\n",
    "        else:\n",
    "            features= get_mask_features_singles(row.sentence, row.token, model_lm, tokenizer)\n",
    "        best_probs.append(features['predicted_prob'])\n",
    "        predicted_words.append(features['predicted_word'])\n",
    "        target_probs.append(features['target_prob'])\n",
    "        \n",
    "    df['predicted_probs'] = best_probs\n",
    "    df['predicted_words'] = predicted_words\n",
    "    df['target_probs'] = target_probs\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = write_mask_features(single_train, model_masked_lm, tokenizer_propercased)\n",
    "single_trial = write_mask_features(single_trial, model_masked_lm, tokenizer_propercased)\n",
    "single_test = write_mask_features(single_test, model_masked_lm, tokenizer_propercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for mwe targets\n",
    "1. ✓ substitute targets for two masks in a sentence\n",
    "2. ✓ get sentence tokens ids\n",
    "3. ✓ get mask ids\n",
    "4. ✓ get logits for masked tokens\n",
    "5. ✓ get max probs for each of masked token predictions\n",
    "6. ✓ get an average of max probs for masked token predictions\n",
    "7. ✓ get probs for each of masked target parts\n",
    "8. ✓ get average of probs for each of masked target parts\n",
    "9. ✓ get predicted tokens\n",
    "10.   see if any token was correctly predcited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_features_mwes(sentence, token_pair, model_lm, tokenizer):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    best_probs\n",
    "    predicted_words\n",
    "    target_prob\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # substitute a target pair with two masks\n",
    "    sentence = sentence.replace(token_pair, tokenizer.mask_token*2)\n",
    "    \n",
    "    # get sentence tokens ids\n",
    "    sentence_tensor = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    \n",
    "    # get mask indices\n",
    "    mask_token_index = torch.where(sentence_tensor == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # prediction for everything\n",
    "    logits = model_lm(sentence_tensor).logits\n",
    "    \n",
    "    # prediction for masked tokens\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "    # probs of masked tokens\n",
    "    probs = torch.nn.functional.log_softmax(mask_token_logits, dim=1) # num_of_masks x vocab\n",
    "    best_probs = probs.max(dim=1)\n",
    "\n",
    "    # prediction as a word\n",
    "    top_1_tokens = torch.topk(mask_token_logits, 1, dim=1).indices\n",
    "    predicted_words = tokenizer.convert_ids_to_tokens(top_1_tokens)\n",
    "    \n",
    "    # check if correct\n",
    "    # get tokens for each of the words\n",
    "    tokenized_targets = [tokenizer.tokenize(token) for token in token_pair.split(' ')]\n",
    "        \n",
    "    pair_probs = [] \n",
    "    for i, part in enumerate(tokenized_targets):\n",
    "        part_probs = []\n",
    "        # ids the ith word of the pair\n",
    "        target_part_ids = tokenizer.convert_tokens_to_ids(part)\n",
    "        for j in target_part_ids: \n",
    "            part_probs.append(probs[i][j].item())\n",
    "        pair_probs.append(sum(part_probs)/len(part_probs))\n",
    "    \n",
    "    features['best_probs'] = best_probs.values.detach().tolist()\n",
    "    features['predicted_words'] = predicted_words\n",
    "    features['pair_probs'] = pair_probs\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "[-3.3664755821228027, -3.8095571994781494, -1.9187053442001343, -0.3489280641078949]\n",
      "['new', 'number', '!', '.']\n",
      "[-9.650083541870117, -8.456759452819824]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'this is a sea horse. hello sea horse'\n",
    "target_word = 'sea horse'\n",
    "\n",
    "features = get_mask_features_mwes(sentence, target_word, model_masked_lm, tokenizer_propercased)\n",
    "print('--------')\n",
    "print(features['best_probs'])\n",
    "print(features['predicted_words'])\n",
    "print(features['pair_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mask_features_mwe(df, model_lm, tokenizer):\n",
    "    \n",
    "    best_probs1=[]\n",
    "    best_probs2=[]\n",
    "    predicted_words = []\n",
    "    target_probs1=[]\n",
    "    target_probs2=[]\n",
    "    best_probs_average=[]\n",
    "    target_probs_average=[]\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        features = get_mask_features_mwes(row.sentence, row.token, model_lm, tokenizer)\n",
    "        best_probs1.append(features['best_probs'][0])\n",
    "        best_probs2.append(features['best_probs'][1])\n",
    "        predicted_words.append(\" \".join(features['predicted_words']))\n",
    "        target_probs1.append(features['pair_probs'][0])\n",
    "        target_probs2.append(features['pair_probs'][1])\n",
    "        best_probs_average.append(np.mean(features['best_probs']))\n",
    "        target_probs_average.append(np.mean(features['pair_probs']))\n",
    "        \n",
    "    df['best_probs1'] = best_probs1\n",
    "    df['best_probs2'] = best_probs2\n",
    "    df['predicted_probs'] = best_probs_average\n",
    "    df['predicted_words'] = predicted_words\n",
    "    df['target_probs1'] = target_probs1\n",
    "    df['target_probs2'] = target_probs2\n",
    "    df['target_probs'] = target_probs_average\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_train = write_mask_features_mwe(multi_train, model_masked_lm, tokenizer_propercased)\n",
    "mwe_trial = write_mask_features_mwe(multi_trial, model_masked_lm, tokenizer_propercased)\n",
    "mwe_test = write_mask_features_mwe(multi_test, model_masked_lm, tokenizer_propercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 WordNet features <a class=\"anchor\" id=\"5\"></a>\n",
    "1. get an average number of synsets that a word is present in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_features(df):\n",
    "    senses = []\n",
    "    for i, row in df.iterrows():\n",
    "        if ' ' in row.token:\n",
    "            s = 0\n",
    "            # look if a pair is present\n",
    "            s += len(wordnet.synsets(row.token.lower().replace(' ','_')))\n",
    "            # look for each word separately\n",
    "            s += sum([len(wordnet.synsets(w)) for w in row.token.lower().split(' ')])\n",
    "            senses.append(s)\n",
    "        else:\n",
    "            syns = wordnet.synsets(row.token.lower())\n",
    "            senses.append(len(syns))\n",
    "    df['senses'] = senses\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = wordnet_features(single_train)\n",
    "single_trial = wordnet_features(single_trial)\n",
    "single_test = wordnet_features(single_test)\n",
    "\n",
    "multi_train = wordnet_features(mwe_train)\n",
    "multi_trial = wordnet_features(mwe_trial)\n",
    "multi_test = wordnet_features(mwe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Word requency features <a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_features(df):\n",
    "    freqs = []\n",
    "    for i, row in df.iterrows():\n",
    "        if ' ' not in row.token:\n",
    "            freqs.append(wordfreq.zipf_frequency(row.token,'en', wordlist='best', minimum=0.0))\n",
    "        else:\n",
    "            tokens = row.token.split()\n",
    "            av_freq = sum([wordfreq.zipf_frequency(token,'en', wordlist='best', minimum=0.0) for token in tokens])/len(tokens)\n",
    "            freqs.append(av_freq)\n",
    "    df['freqs'] = freqs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = freq_features(single_train)\n",
    "single_trial = freq_features(single_trial)\n",
    "single_test = freq_features(single_test)\n",
    "\n",
    "multi_train = freq_features(multi_train)\n",
    "multi_trial = freq_features(multi_trial)\n",
    "multi_test = freq_features(multi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Morfessor features <a class=\"anchor\" id=\"7\"></a>\n",
    "1. get frequencies of subwords\n",
    "2. segment target sentence and target\n",
    "3. get average freq\n",
    "4. get average len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morfessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0212 15:49:13.236800 47459466215040 io.py:186] Loading model from 'data/english_model'...\n",
      "I0212 15:49:13.963269 47459466215040 io.py:188] Done.\n"
     ]
    }
   ],
   "source": [
    "io = morfessor.MorfessorIO()\n",
    "segmentation_model = io.read_binary_model_file('data/english_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructions = segmentation_model.get_constructions()\n",
    "probs_dict = {}\n",
    "for c in constructions:\n",
    "    probs_dict[c[0]] = np.log(c[1]/segmentation_model.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morfessor_features(df, seg_model, prob_dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    len_target_subs = []\n",
    "    prob_subs = []\n",
    "    for i,row in df.iterrows():\n",
    "            \n",
    "        if \" \" not in row.token:\n",
    "            if row.id =='38LRF35D5LWPYKNDAPAKMD6HD1M3UI':\n",
    "                segments = seg_model.viterbi_segment(row.token.lower())[0]\n",
    "            else:\n",
    "                segments = seg_model.viterbi_segment(row.token)[0]\n",
    "            len_target_subs.append(len(segments))\n",
    "            probs = sum([prob_dict[sub] for sub in segments])/len(segments)\n",
    "            prob_subs.append(probs)\n",
    "        else:\n",
    "            tokens = row.token.split(\" \")\n",
    "            segments = []\n",
    "            for token in tokens:\n",
    "                segments+=seg_model.viterbi_segment(token)[0]\n",
    "            len_target_subs.append(len(segments))\n",
    "            probs = sum([prob_dict[sub] for sub in segments])/len(segments)\n",
    "            prob_subs.append(probs)\n",
    "    \n",
    "    df['morfessor_len'] = len_target_subs\n",
    "    df['morfessor_freqs'] = prob_subs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = get_morfessor_features(single_train, segmentation_model, probs_dict)\n",
    "single_trial = get_morfessor_features(single_trial, segmentation_model, probs_dict)\n",
    "single_test = get_morfessor_features(single_test, segmentation_model, probs_dict)\n",
    "\n",
    "multi_train = get_morfessor_features(multi_train, segmentation_model, probs_dict)\n",
    "multi_trial = get_morfessor_features(multi_trial, segmentation_model, probs_dict)\n",
    "multi_test = get_morfessor_features(multi_test, segmentation_model, probs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 saving non-embedding features <a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id', 'corpus', 'sentence', 'token', 'complexity', 'sentence_len', 'tokens_per_word', 'target_len',\n",
    "       'target_len_chars', 'best_probs1', 'best_probs2', 'predicted_words',\n",
    "       'target_probs1', 'target_probs2', 'predicted_probs',\n",
    "       'target_probs', 'senses', 'freqs', 'morfessor_len', 'morfessor_freqs']\n",
    "\n",
    "multi_train[columns].to_csv('features/multi_train.csv')\n",
    "multi_trial[columns].to_csv('features/multi_trial.csv')\n",
    "multi_test[columns].to_csv('features/multi_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['id', 'corpus', 'sentence', 'token', 'complexity', 'sentence_len', 'tokens_per_word', 'target_len',\n",
    "       'target_len_chars', 'predicted_probs', 'predicted_words',\n",
    "       'target_probs', 'senses', 'freqs','morfessor_len', 'morfessor_freqs']\n",
    "\n",
    "single_train[columns].to_csv('features/single_train.csv')\n",
    "single_trial[columns].to_csv('features/single_trial.csv')\n",
    "single_test[columns].to_csv('features/single_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('diplomchik': conda)",
   "language": "python",
   "name": "python36664bitdiplomchikcondac9dda3a2509d43ad9b71af24f3396da7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
